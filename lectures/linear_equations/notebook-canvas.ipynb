{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Linear Equations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline \n",
    "\n",
    "1. Setup\n",
    "2. Special cases\n",
    "3. L-U Factorization\n",
    "4. Pivoting\n",
    "5. Benchmarking\n",
    "6. Ill conditioning\n",
    "7. Iterative methods\n",
    "8. Resources\n",
    "\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear equation is the most elementary problem that arises in computational economic analysis. In a linear equation, an $n \\times n$ matrix $A$ and an n-vector $b$ are given, and one must compute the $n$-vector $x$ that satisfies\n",
    "$Ax = b$.\n",
    "\n",
    "Linear equations arise naturally in many  economic applications such as\n",
    "\n",
    "- Linear multicommodity market equilibrium models\n",
    "- Finite-state financial market models \n",
    "- Markov chain models \n",
    "- Ordinary least squares\n",
    "\n",
    "They more commonly arise indirectly from numerical solution to nonlinear and functional equations:\n",
    "\n",
    "- Nonlinear multicommodity market models\n",
    "- Multiperson static game models\n",
    "- Dynamic optimization models\n",
    "- Rational expectations models \n",
    "\n",
    "Applications often require the repeated solution of very large linear equation systems. In these situations, issues regarding speed, storage requirements, and preciseness of the solution of such equations can arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from linear_algorithms import backward_substitution\n",
    "from linear_algorithms import forward_substitution\n",
    "from linear_algorithms import gauss_seidel\n",
    "from linear_algorithms import solve\n",
    "\n",
    "from linear_plots import plot_operation_count\n",
    "\n",
    "from linear_problems import get_ill_problem_1\n",
    "from linear_problems import get_inverse_demand_problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Special cases\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start with some special cases to develop a basic understanding for the core building blocks for more complicated settings. Let's start with the case of a lower triangular matrix $A$, where we can solve the linear equation by a simple backward or forward substitution. Let's consider the following setup.\n",
    "\n",
    "$$\n",
    "A  =  \\begin{bmatrix}\n",
    "   a_{11} & 0      & 0 \\\\\n",
    "   a_{21} & a_{22} & 0 \\\\\n",
    "   a_{31}   & a_{32} & a_{33} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Consider an algorithmic implementation of forward-substitution as an example.\n",
    "\n",
    "$$\n",
    "x_i = \\left ( b_i - \\sum_{j=1}^{i-1} a_{ij}x_j \\right )/a_{ii}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution:  [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "def forward_substitution(a, b):\n",
    "\n",
    "    x = np.zeros_like(b, dtype=np.double)\n",
    "    \n",
    "    # Initialiize first row\n",
    "    x[0] = b[0] / a[0, 0]\n",
    "    \n",
    "    for i in range(1, a.shape[0]):\n",
    "        x[i] = (b[i] - np.dot(a[i, :i], x[:i])) / a[i, i]\n",
    "    return x\n",
    "    \n",
    "A = np.identity(3)\n",
    "b = np.array([1, 1, 1])\n",
    "\n",
    "\n",
    "sol = forward_substitution(A, b)\n",
    "print(\"Solution: \", sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_problem():\n",
    "    \n",
    "    size = np.random.randint(1, 5)\n",
    "    \n",
    "    A = np.tril(np.random.normal(size=(size, size)))\n",
    "    x = np.random.normal(size=size)\n",
    "    b = np.matmul(A, x)\n",
    "    \n",
    "    return A, b, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "for _ in range(10):\n",
    "    A, b, x = test_problem()\n",
    "    x_sol = forward_substitution(A, b)\n",
    "    np.testing.assert_almost_equal(x, x_sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Questions_ \n",
    "     \n",
    "* How can we make the test code more generic and sample test problems of different dimensions?\n",
    "* Is there a way to control the randomness in the test function?\n",
    "* Is there software out there that allows to automate parts of the testing?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have an upper triangular matrix, we can use backward substitution to solve the linear system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mbackward_substitution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mbackward_substitution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Perform backward substitution to solve a system of linear equations.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Solves a linear equation of type :math:`Ax = b` when for an *upper triangular* matrix\u001b[0m\n",
       "\u001b[0;34m    :math:`A` of dimension :math:`n \\\\times n` and vector :math:`b` of length :math:`n`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Parameters\u001b[0m\n",
       "\u001b[0;34m    ----------\u001b[0m\n",
       "\u001b[0;34m    a : numpy.ndarray\u001b[0m\n",
       "\u001b[0;34m        Lower triangular matrix of dimension :math:`n \\\\times n`.\u001b[0m\n",
       "\u001b[0;34m    b : numpy.ndarray\u001b[0m\n",
       "\u001b[0;34m        Vector of length :math:`n`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Returns\u001b[0m\n",
       "\u001b[0;34m    -------\u001b[0m\n",
       "\u001b[0;34m    x : numpy.ndarray\u001b[0m\n",
       "\u001b[0;34m        Solution of the linear equations. Vector of length :math:`n`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# Get number of rows.\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtmp\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/external-storage/sciebo/office/OpenSourceEconomics/teaching/scientific-computing/course/lectures/linear_equations/linear_algorithms.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??backward_substitution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Exercise_ \n",
    "\n",
    "\n",
    "* Implement the same testing setup as above the backward-substitution function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build on these two functions to tackle more complex tasks. This is a good example on how to develop scientific software step-by-step ensuring that each component is well tested before integrating into more involved settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## L-U Factorization\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most linear equations encountered in practice, however, do not have a triangular $A$ matrix. Doolittle and Crout have shown that any matrix $A$ can be decomposed into the product of a (row-permuted) lower and upper triangular matrix $L$ and $U$, respectively $A=L \\times U$ using **Gaussian elimination**. We will not look into the Gaussian elimination algorithm, but there is an example application in our textbook where you can follow along step by step. The L-U algorithm is designed to decompose the $A$ matrix into the product of lower and upper triangular matrices, allowing the linear equation to be solved using a combination of backward and forward substitution. \n",
    "\n",
    "Here are the two core steps:\n",
    "\n",
    "* Factorization phase\n",
    "\n",
    "\\begin{align*}\n",
    "A = LU\n",
    "\\end{align*}\n",
    "\n",
    "* Solution phase:\n",
    "\n",
    "\\begin{align*}\n",
    "Ax = (LU)x=L(Ux) = b, \n",
    "\\end{align*}\n",
    "\n",
    "where we solve $Ly = b$ using forward-substitution and $Ux=y$ using backward-substitution.\n",
    "\n",
    "Adding to this the two building blocks we developed earlier `forward_substitution` and `backward_substitution`, we can now write a quite generic function to solve systems of linear equations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this is actually working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting\n",
    "\n",
    "\n",
    "Rounding error can cause serious error when solving linear equations. Let's consider the following example, where $\\epsilon$ is a tiny number.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \\epsilon & 1\\\\ 1 & 1 \\end{bmatrix} \\times \\left[ \\begin{array}{c} x_1 \\\\ x_2 \\end{array} \\right] = \\left[\\begin{array}{c} 1 \\\\ 2 \\end{array} \\right] \n",
    "$$\n",
    "\n",
    "\n",
    "It is easy to verify that the right solution is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1 & = \\frac{1}{1 - \\epsilon} \\\\\n",
    "x_2 & = \\frac{1 - 2 \\epsilon}{1 - \\epsilon}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and thus $x_1$ is slightly more than one and $x_2$ is slightly less than one. To solve the system using Gaussian elimination we need to add $-1/\\epsilon$ times the first row to the second row. We end up with\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \\epsilon & 1 \\\\ 0 & 1 - \\frac{1}{\\epsilon} \\end{bmatrix} \\times \\left[ \\begin{array}{c} x_1 \\\\ x_2 \\end{array} \\right] = \\left[ \\begin{array}{c} 1 \\\\ 2 - \\frac{1}{\\epsilon} \\end{array} \\right],\n",
    "$$\n",
    "\n",
    "which we can then solve recursively.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_2 & = \\frac{2 - 1/\\epsilon}{1 - 1/\\epsilon} \\\\\n",
    "x_1 & = \\frac{1 - x_2}{\\epsilon}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's translate this into code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our solution algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we have to realize that the results are grossly inaccurate.  What happened? Is there any hope to apply `numpy`'s routine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm  does automatically check whether such rounding errors can be avoided by simply changing the order of rows. This is called **pivoting** and changes the recursive solution to\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_2 & = \\frac{1 - 2\\epsilon}{1 - \\epsilon} \\\\\n",
    "x_1 & = 2 - x_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which can be solved more accurately. Our implementation also solves the modified problem well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building your own numerical routines is usually the only way to really understand the algorithms and learn about all the potential pitfalls. However, the default should be to rely on battle-tested production code. For linear algebra there are numerous well established libraries available. Building your own numerical routines is usually the only way to really understand the algorithms and learn about all the potential pitfalls. However, the default should be to rely on battle-tested production code. For linear algebra there are numerous well established libraries available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Benchmarking\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does solving a system of linear equations by an $L-U$ decomposition compare to other alternatives of solving the system of linear equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right setup for your numerical needs depends on your particular problem. For example, this trade-off looks very different if you have to solve numerous linear equations that only differ in $b$ but not $A$. In this case you only need to compute the inverse once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Exercise_ \n",
    "   \n",
    "\n",
    "* Set up a benchmarking exercise that compares the time to solution for the two approaches for $m=\\{1, 100\\}$ and $n = \\{50, 100\\}$, where $n$ denotes the number of linear equations and $m$ the number of repeated solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ill Conditioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some linear equations are inherently difficult to solve accurately on a computer. This difficulty occurs when the A matrix is structured in such a way that a small perturbation $\\delta b$ in the data vector $b$ induces a large change $\\delta x$ in the solution vector $x$. In such cases the linear equation or, more generally, the $A$ matrix is said to be **ill conditioned**.\n",
    "\n",
    "One measure of ill conditioning in a linear equation Ax = b is the “elasticity” of the solution vector $x$ with respect to the data vector $b$ \n",
    "\n",
    "$$\n",
    "\\epsilon = \\sup_{||\\delta  b|| > 0} \\frac{||\\delta x|| / ||x||}{||\\delta b|| / ||b||}\n",
    "$$\n",
    "\n",
    "The elasticity gives the maximum percentage change in the size of the solution vector $x$ induced by a $1$ percent change in the size of the data vector $b$. If the elasticity is large, then small errors in the computer representation of the data vector $b$ can produce large errors in the computed solution vector x. Equivalently, the computed solution $x$ will have far fewer significant digits than the data vector $b$.\n",
    "\n",
    "In practice, the elasticity is estimated using the condition number of the matrix $A$, which for invertible $A$ is defined by $\\kappa \\equiv ||A|| \\cdot ||A^{-1} ||$. The condition number is always greater than or equal to one. Numerical analysts often use the rough rule of thumb that for each power of $10$ in the condition number, one significant digit is lost in the computed solution vector $x$. Thus, if $A$ has a condition number of $1,000$, the computed solution vector $x$ will have about three fewer significant digits than the data vector $b$.\n",
    "\n",
    "Let's look at an example, where the solution vector is all ones but but the linear equation is notoriously ill-conditioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the solution error depend on the condition number in this setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a more general lesson here. Always be skeptical about the quality of your numerical results. See these two papers for an exploratory analysis of econometric software packages and nonliner optimization. Yes, they are rather old and much progress has been made, but the general points remain valid.\n",
    "\n",
    "* https://www.aeaweb.org/articles?id=10.1257/000282803322157133\n",
    "\n",
    "* https://www.aeaweb.org/articles?id=10.1257/jel.37.2.633"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Exercise_\n",
    "\n",
    "Let's consider the following example as well, which is taken from Johansson (2015).\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{c} 1 \\\\ 2 \\end{array}\\right] = \\begin{bmatrix} 1 & \\sqrt{p}\\\\ 1 & \\frac{1}{\\sqrt{p}} \\end{bmatrix} \\times \\left[ \\begin{array}{c} x_1 \\\\ x_2 \\end{array} \\right]\n",
    "$$\n",
    "\n",
    "This system is singular for $p=1$ and for $p$ in the vicinity of one is ill-conditioned.\n",
    "\n",
    "* Create two plots that show the condition number and the error between the analytic and numerical solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Iterative methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms based on Gaussian elimination are called exact or, more properly, direct methods because they would generate exact solutions for the linear equation $Ax = b$ after a finite number of operations, if not for rounding error. Such methods are ideal for moderately sized linear equations but may be impractical for large ones. Other methods, called iterative methods, can often be used to solve large linear equations more efficiently if the $A$ matrix is sparse, that is, if $A$ is composed mostly of zero entries. Iterative methods are designed to generate a sequence of increasingly accurate approximations to the solution of a linear equation, but they generally do not yield an exact solution after a prescribed number of steps, even in theory.\n",
    "\n",
    "\n",
    "The most widely used iterative methods for solving a linear equation $Ax = b$ are developed by choosing an easily invertible matrix $Q$ and writing the linear equation in the equivalent form\n",
    "\n",
    "$$\n",
    "Qx = b + (Q - A)x\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "x = Q^{-1} b + (I - Q^{-1} A)x\n",
    "$$\n",
    "\n",
    "This form of the linear equation suggests the iteration rule \n",
    "\n",
    "$$\n",
    "x^{k+1}\\leftarrow Q^{-1} b + (I - Q^{-1} A)x^{k}\n",
    "$$\n",
    "\n",
    "which, if convergent, must converge to a solution of the linear equation. Ideally, the so-called splitting matrix $Q$ will satisfy two criteria. First, $Q^{-1}b$ and $Q^{-1} A$ should be relatively easy to compute. This criterion is met if $Q$ is either diagonal or triangular. There are two popular approaches:\n",
    "\n",
    "* The **Gauss-Seidel** method sets $Q$ equal to the upper triangular matrix formed from the upper triangular elements of $A$.\n",
    "\n",
    "* The **Gauss-Jacobi** method sets $Q$ equal to the diagonal matrix formed from the diagonal entries of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Exercise_ \n",
    "\n",
    "* Implement the Gauss-Jacobi method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's conclude with an economic application as outlined in Judd (1998). Suppose we  have the following inverse demand function $p = 10 - q$ and the following supply curve $q = p / 2 +1$. Equilibrium is where supply equals demand and thus we need to solve the following linear system.\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{c} 10 \\\\ -2 \\end{array}\\right] = \\begin{bmatrix} 1 & 1\\\\ 1 & -2\\end{bmatrix} \\times \\left[ \\begin{array}{c} p \\\\ q \\end{array} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compre the two solution approaches and make sure that they in fact give the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Software\n",
    "\n",
    "* **The PARDISO Solver Project**: https://www.pardiso-project.org\n",
    "\n",
    "* **LAPACK — Linear Algebra PACKage**: http://www.netlib.org/lapack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "- Robert Johansson. *Numerical Python: scientific computing and data science applications with NumPy, SciPy and Matplotlib*. Apress, 2018.\n",
    "\n",
    "- William H Press, Brian P Flannery, Saul A Teukolsky, and William T Vetterling. *Numerical recipes: The art of scientific computing*. Cambridge University Press, 1986.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
