{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Nonlinear Equations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. Setup\n",
    "2. Bisection method\n",
    "3. Function iteration\n",
    "4. Newton's method\n",
    "5. Quasi-Newton method\n",
    "6. Choosing a solution method\n",
    "7. Convergence\n",
    "8. Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nonlinear equation takes one of two forms.\n",
    "\n",
    "* **Rootfinding problem** Given a function $f: R^n \\mapsto R^n$,compute an $n$-vector $x^*$, called root of $f$, such that $f(x^*) = 0$.\n",
    "\n",
    "* **Fixed-point problem** Given a function $g: R^n \\mapsto R^n$, compute an $n$-vector $x^*$ called fixed point of $g$ such that $g(x^*) = x^*$\n",
    "\n",
    "The two forms are equivalent.\n",
    "\n",
    "* A root of $f$ is a fixed-point of $g(x) = x - f(x)$.\n",
    "* A fixed-point of $g$ is a root ot $f(x) = x - g(x)$.\n",
    "\n",
    "Nonlinear equations arise naturally in economics:\n",
    "\n",
    "* Multicommodity market equilibrium models\n",
    "* Multiperson static game models \n",
    "* Unconstrained optimization models\n",
    "\n",
    "Nonlinear equations also arise indirectly when numerically solving economic models involving functional equations:\n",
    "\n",
    "* Dynamic optimization models \n",
    "* Rational expectations models \n",
    "* Arbitrage pricing models\n",
    "\n",
    "Building on our earlier lecture on the solution to linear systems of equations, often the solution to a nonlinear problem is computed iteratively by solving a sequence of linear problems. \n",
    "\n",
    "* sensitivity to intial condition\n",
    "* susceptible to rounding error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from temfpy.nonlinear_equations import exponential  # noqa: F401\n",
    "from scipy import optimize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nonlinear_algorithms import bisect\n",
    "from nonlinear_plots import plot_bisection_test_function\n",
    "\n",
    "from nonlinear_algorithms import fixpoint\n",
    "from nonlinear_plots import plot_fixpoint_example\n",
    "from nonlinear_plots import plot_newton_pathological_example\n",
    "from nonlinear_plots import plot_convergence\n",
    "from nonlinear_plots import plot_newtons_method\n",
    "from nonlinear_plots import plot_secant_method\n",
    "\n",
    "from nonlinear_algorithms import newton_method\n",
    "\n",
    "from nonlinear_problems import bisection_test_function\n",
    "from nonlinear_problems import function_iteration_test_function\n",
    "from nonlinear_problems import newton_pathological_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bisection method\n",
    "\n",
    "We start with the implementation (and testing) of the bisection algorithm.\n",
    "\n",
    "\n",
    "* The bisection method is perhaps the simplest and most robust method for computing the root of a continuous real-valued function defined on a bounded interval of the real line. The bisection method is based on the Intermediate Value Theorem, which asserts that if a continuous real-valued function defined on an interval assumes two distinct values, then it must assume all values in between. In particular, if $f$ is continuous, and $f (a)$ and $f (b)$ have different signs, then $f$ must have at least one root $x$ in $[a, b]$.\n",
    "\n",
    "* Each iteration begins with an interval known to contain or to bracket a root of $f$, because the function has different signs at the interval endpoints. The interval is bisected into two subintervals of equal length. One of the two subintervals must have endpoints of different signs and thus must contain a root of $f$. This subinterval is taken as the new interval with which to begin the subsequent iteration. In this manner, a sequence of intervals is generated, each half the width of the preceding one, and each known to contain a root of $f$. The process continues until the width of the bracketing interval containing a root shrinks below an acceptable convergence tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a test function to get a sense of what result can should expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The ability to vectorize functions using [np.vectorize](https://numpy.org/doc/stable/reference/generated/numpy.vectorize.html) comes in very handy when evaluating functions over a grid. However, be aware that this is a simple replacement of a `for loop` and thus does not improve performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to go ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to check our implementation and investigate the sensitivity of results to alternative tuning parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Exercises_ \n",
    "\n",
    "1. Write a short test that ensures that in fact found a root of the function. \n",
    "2. Create a simple plot that shows each iterate of $x$ and label the two axis appropriately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Function iteration\n",
    "\n",
    "\n",
    "* Function iteration is a relatively simple technique that may be used to compute a fixed point $x = g(x)$ of a function from $R^n$ to $R^n$. The technique is also applicable to a rootfinding problem $f(x) = 0$ by recasting it as the equivalent fixed-point problem $g(x) = x - f (x)$. Function iteration begins with the analyst supplying a guess $x^{(0)}$ for the fixed point of $g$. Subsequent iterates are generated using the simple iteration rule\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x^{(k+1)} \\leftarrow g(x^{(k)})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since $g$ is continuous, if the iterates converge, they converge to a fixed point of $g$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a test function to get a sense of what result can should expect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run and test our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is an example of using a try-except block to explicitly handle the `AssertionError`. Details on exception handling in Python, see this [Wiki](https://wiki.python.org/moin/HandlingExceptions) for some more examples. \n",
    "\n",
    "We set two parameters manually, which one is it? How about choosing different starting values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `tolerance` setting instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Exercises_\n",
    "\n",
    "1. Find the fixpoint of $g(x) = \\sqrt{x + 0.2}$. \n",
    "2. How many iterations does the function iteration need? Does it depend on the starting value? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Newton's method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton's method is an algorithm for computing the root of a function $f : R^n \\mapsto R^n$. Newton's method employs a strategy of successive linearization. The strategy calls for the nonlinear function $f$ to be approximated by a sequence of linear functions whose roots are easily computed and, ideally, converge to the root of $f$. In particular, the $k + 1^{th}$ iterate \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_{k+1} = x_k - f^\\prime(x_k)^{-1}f(x_k) \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "is the root of the Taylor linear approximation of $f$ around the preceding iterate $x_k$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x) \\approx f(x_k) + f^\\prime(x_k)(x - x_k).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This yields the following iteration rule:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_{k + 1} = x_k - f^\\prime(x_k)^{-1} f(x_k)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If $n = 1$, the iteration rule takes the simpler form \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_{k + 1} = x_k - \\frac{f(x_k)}{f^\\prime(x_k)}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Exercises_\n",
    "\n",
    "1. Plot the function $f(x) = x^4 - 2$ and provide a rough estimate of its fix point. \n",
    "2. Let's compute the root of the function using Newton's method by employing the proper iteration rule: \n",
    "$$\\begin{align*}\n",
    "x_{k + 1} = x_k - \\frac{f(x_k)}{f^\\prime(c_k)} = x_k - \\frac{x^4_k - 2}{4x^3_k}\n",
    "\\end{align*}$$\n",
    "      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at a more general implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Question_\n",
    "\n",
    "* What are the important generalizations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let' explore one last test function to test the interface of our function and learn something about [private functions](https://dbader.org/blog/meaning-of-underscores-in-python) in the process.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x) = x^3 - 2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A potential shortcoming of Newton's method  is that the derivatives required for the Jacobian may not be available may be difficult to calculate analytically, or time-consuming to approximate numerically ... or that it might actually fail or result in cycles.\n",
    "\n",
    "Consider the following pathological example which has a unique root at $0$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x) = x^{\\frac{1}{3}} \\, e^{-x^2}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can derive the Newton iterates as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_{k + 1} = x_k\\left(1 - \\frac{3}{1 - 6 x_k^2 }\\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Questions_\n",
    "\n",
    "1. What happens when you apply Newton's method to solve this function for different starting values?\n",
    "2. What does the Newton iterate look like for very small and very large values?\n",
    "\n",
    "\n",
    "* For $x_k$ small, the iteration reduces to $x_{k + 1} = -2x_k$ and so it converges to $0$ only if $x_0 = 0$.\n",
    "\n",
    "* For $x_k$ large, the iteration becomes \n",
    "\n",
    "\\begin{align*}\n",
    "x_{k + 1} = x_k \\left(1 + \\frac{3}{x_k^2}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "which diverges, but will eventually satisfy the stopping rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another problems might be cycles.\n",
    "\n",
    "<img src=\"material/fig-newton-cycles.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Quasi-Newton method\n",
    "\n",
    "\n",
    "Quasi-Newton methods replace the Jacobian in Newton’s method with an estimate that is easier to compute. Specifically, quasi-Newton methods use an iteration rule\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_{k+1} = x_k - A^{-1}_k f(x_k)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $A_k$ is an estimate of the Jacobian $f^\\prime(x_k)$. \n",
    "\n",
    "The **secant method** replaces the derivative in Newton’s method with the estimate\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f^\\prime(x_k) \\approx \\frac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The secant method is so called because it approximates the function $f$ using *secant* lines drawn through successive pairs of points on its graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a crude implementation to make sure we understood the setup correctly. We will use this to show the `lambda` functions, see [here](https://realpython.com/python-lambda) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Broyden’s method** is the most popular multivariate generalization of the univariate secant method. **Broyden’s method** replaces the Jacobian in Newton’s method with an estimate $A_k$ that is updated by making the smallest possible change (measured by the Frobenius norm) that is consistent with the secant condition:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x_{k + 1}) - f(x_k) = A_{k+1} (x_{k+1} - x_k ).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This yields the iteration rule\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A_{k+1} = A_k + (f(x_{k+1}) - f(x_k) - A_k d_k)\\frac{d^\\prime_k}{d^\\prime_k d_k}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $d_k = x_{k+1} - x_k$. Often $A_0$ is equal to the numerical approximation of $f$ at $x_0$. The remarkable feature of Broyden's method is that it is able to generate a reasonable approximation to the Jacobian matrix with no additional evaluations of the function. This approach is readily available in [scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.broyden1.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cournot problem\n",
    "\n",
    "Consider a market with two firms producing the same good. Firm $i$'s total cost of production is a function of the quantity $q_i$ it produces.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "C_i(q_i) = \\frac{\\beta_i}{2} q_i^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The market clearing price is a function of the total quantity produced by both firms.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(q_1 + q_2) = (q_1 - q_2)^{-\\alpha}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Firm $i$ chooses production $q_i$ so as to maximize its profit taking the other firm's output as given.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\pi_i (q_1 , q_2 ) = P (q_1 + q_2 )q_i - C_i (q_i ).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus in equilibrium,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\pi_i}{\\partial q_i} = (q_1 - q_2)^{-\\alpha} - \\alpha (q_1 + q_2)^{-(\\alpha + 1) } q_i  - \\beta_i q_i = 0\\qquad\\text{for}\\quad i = 1, 2.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Exercise_\n",
    "\n",
    "* Compute the market equilibrium quantities using Broyden's method for $\\alpha =  0.6$ and $\\beta= [0.6, 0.8]$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"material/fig-convergence-path.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Choosing a solution method\n",
    "\n",
    "\n",
    "\n",
    "We now consider a more challenging task and compare the performance of `scipy`'s root finding algorithms. We will use one of [temfpy](https://temfpy.readthedocs.io/en/latest/index.html)'s test functions for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see which of the algorithms performs best for a ten dimensional problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to run our benchmarking exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can explore the performance of the alternative implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Convergence\n",
    "        \n",
    "\n",
    "Two factors determine the speed with which a properly coded and initiated algorithm will converge to a solution:\n",
    "\n",
    "* Asymptotic rate of convergence\n",
    "* Computational effort per iteration\n",
    "\n",
    "The asymptotic rate of convergence measures improvement afforded per iteration near the solution. A sequence $x_k$ converges to $x^*$ at an asymptotic rate of order $p$ if there is constant $C > 0$ such that for $k$ sufficiently large,\n",
    "\n",
    "$$\n",
    "\\begin{align*}|\n",
    "||x_{k+1} - x^* || \\leq C ||x_k - x^* ||^p.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "* Function iteration converges at a “linear” rate with $p = 1$ and $C < 1$. \n",
    "* Broyden’s method converges at a “superlinear” rate with $p \\approx 1.62$.\n",
    "* Newton’s method converges at a “quadratic” rate with $p = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, algorithms differ in computations per iteration.\n",
    "\n",
    "* Function iteration requires a function evaluation.\n",
    "* Broyden’s method additionally requires a linear solve.\n",
    "* Newton’s method additionally requires a Jacobian evaluation.\n",
    "\n",
    "Thus, a faster rate of convergence typically can be achieved only by investing greater computational effort per iteration. The optimal tradeoff between rate of convergence and computational effort per iteration varies across applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "#### Software\n",
    "\n",
    "* **HOMPACK**: https://www.netlib.org/hompack"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
